{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Pattern with Local Ollama Model\n",
    "\n",
    "ReAct (Reasoning + Acting) 패턴을 **로컬 Ollama 모델**을 사용하여 구현합니다.\n",
    "\n",
    "## 사전 요구사항\n",
    "1. [Ollama](https://ollama.ai/) 설치\n",
    "2. 모델 다운로드: `ollama pull gemma3:4b`\n",
    "3. Ollama 서버 실행 중 (기본: http://localhost:11434)\n",
    "\n",
    "## 지원 모델 (도구 호출 지원)\n",
    "- `gemma3:4b` (추천 - 경량, 빠름)\n",
    "- `llama3.1`\n",
    "- `llama3.2`\n",
    "- `mistral`\n",
    "- `qwen2.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama URL: http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Ollama URL 설정 (기본값: localhost:11434)\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "print(f\"Ollama URL: {OLLAMA_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama 서버 연결 성공!\n",
      "사용 가능한 모델: ['candiy:4b', 'candiy:12b', 'bge-m3:latest', 'sakak:8b', 'nomic-embed-text:latest', 'gemma3:12b', 'gemma3:4b', 'gemma3:1b', 'candiy:8b', 'candiy-insurance:8b', 'candiy:2b', 'aya:8b', 'llama3.2:1b', 'gemma2:2b']\n"
     ]
    }
   ],
   "source": [
    "# Ollama 서버 연결 확인\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get(\"models\", [])\n",
    "        print(\"Ollama 서버 연결 성공!\")\n",
    "        print(f\"사용 가능한 모델: {[m['name'] for m in models]}\")\n",
    "    else:\n",
    "        print(f\"Ollama 서버 응답 오류: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Ollama 서버에 연결할 수 없습니다.\")\n",
    "    print(\"Ollama가 실행 중인지 확인해주세요: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 도구(Tools) 정의\n",
    "\n",
    "ReAct 에이전트가 사용할 도구들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의된 도구: ['add', 'multiply', 'divide', 'get_weather']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide the first number by the second number.\"\"\"\n",
    "    if b == 0:\n",
    "        return \"Error: Cannot divide by zero.\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a city. (Simulation)\"\"\"\n",
    "    weather_data = {\n",
    "        \"seoul\": \"Sunny, 15°C\",\n",
    "        \"busan\": \"Cloudy, 18°C\",\n",
    "        \"jeju\": \"Rainy, 20°C\",\n",
    "        \"daejeon\": \"Sunny, 16°C\",\n",
    "        \"new york\": \"Cloudy, 10°C\",\n",
    "        \"tokyo\": \"Sunny, 22°C\",\n",
    "    }\n",
    "    return weather_data.get(city.lower(), f\"Weather data not found for {city}.\")\n",
    "\n",
    "\n",
    "# 사용할 도구 목록\n",
    "tools = [add, multiply, divide, get_weather]\n",
    "print(f\"정의된 도구: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM 설정 (Local Ollama)\n",
    "\n",
    "도구 호출을 지원하는 로컬 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma3:4b 모델 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 사용할 모델 선택 (도구 호출 지원 모델)\n",
    "MODEL_NAME = \"gemma3:4b\"  # 또는 \"llama3.1\", \"mistral\", \"qwen2.5\" 등\n",
    "\n",
    "# Ollama 모델 초기화\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=OLLAMA_URL,\n",
    "    temperature=0,  # 일관된 출력을 위해 0으로 설정\n",
    ")\n",
    "\n",
    "# 도구를 바인딩한 LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(f\"{MODEL_NAME} 모델 설정 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangGraph를 사용한 ReAct 에이전트 구현\n",
    "\n",
    "### 4.1 방법 1: `create_react_agent` 사용 (간단한 방법)\n",
    "\n",
    "> **Note**: LangGraph v1.0에서 deprecated 경고가 나오지만 정상 작동합니다. v2.0에서 제거될 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ReAct 에이전트 생성 (deprecated 경고가 나오지만 정상 작동)\n",
    "react_agent = create_react_agent(llm, tools)\n",
    "\n",
    "print(\"ReAct 에이전트 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 테스트 (영어 사용 권장 - 로컬 모델은 영어 성능이 더 좋음)\n",
    "response = react_agent.invoke({\"messages\": [(\"human\", \"Add 3 and 5, then multiply the result by 2\")]})\n",
    "\n",
    "# 결과 출력\n",
    "for message in response[\"messages\"]:\n",
    "    print(f\"[{message.type}]: {message.content}\")\n",
    "    if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "        print(f\"  -> Tool calls: {message.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 방법 2: 수동으로 ReAct 그래프 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "# 에이전트 노드\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"에이전트가 추론하고 다음 행동을 결정합니다.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 조건부 라우팅\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"도구를 호출해야 하는지 결정합니다.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# 도구 노드\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 그래프 구성\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "custom_react_agent = workflow.compile()\n",
    "\n",
    "print(\"수동 ReAct 그래프 구성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 시각화\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(custom_react_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"그래프 시각화 실패: {e}\")\n",
    "    print(\"그래프 구조: START -> agent -> (tools -> agent) | END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 테스트 케이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str, agent=react_agent, verbose: bool = True):\n",
    "    \"\"\"에이전트를 실행하고 결과를 출력합니다.\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    response = agent.invoke({\"messages\": [(\"human\", query)]})\n",
    "    \n",
    "    if verbose:\n",
    "        for msg in response[\"messages\"]:\n",
    "            if msg.type == \"human\":\n",
    "                continue\n",
    "            elif msg.type == \"ai\":\n",
    "                if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                    print(f\"\\n[AI - Tool Calls]\")\n",
    "                    for tc in msg.tool_calls:\n",
    "                        print(f\"  -> {tc['name']}({tc['args']})\")\n",
    "                elif msg.content:\n",
    "                    print(f\"\\n[AI - Final Response]\")\n",
    "                    print(f\"  {msg.content}\")\n",
    "            elif msg.type == \"tool\":\n",
    "                print(f\"\\n[Tool Result] {msg.name}: {msg.content}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 1: 수학 연산 (영어)\n",
    "run_agent(\"Add 10 and 20, then divide the result by 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 2: 날씨 정보\n",
    "run_agent(\"What is the weather in Seoul and Tokyo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 3: 복합 작업\n",
    "run_agent(\"Multiply 7 by 8, then add 10 to the result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 한국어 테스트 (선택사항)\n",
    "\n",
    "일부 로컬 모델은 한국어 지원이 제한적일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 테스트 (모델에 따라 결과가 다를 수 있음)\n",
    "try:\n",
    "    run_agent(\"5와 10을 더해줘\")\n",
    "except Exception as e:\n",
    "    print(f\"한국어 처리 중 오류: {e}\")\n",
    "    print(\"영어로 시도해보세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 스트리밍 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_stream(query: str, agent=react_agent):\n",
    "    \"\"\"에이전트를 스트리밍 모드로 실행합니다.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for event in agent.stream({\"messages\": [(\"human\", query)]}, stream_mode=\"updates\"):\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n[{node_name} node]\")\n",
    "            if \"messages\" in node_output:\n",
    "                for msg in node_output[\"messages\"]:\n",
    "                    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                        for tc in msg.tool_calls:\n",
    "                            print(f\"  -> Tool call: {tc['name']}({tc['args']})\")\n",
    "                    elif hasattr(msg, \"content\") and msg.content:\n",
    "                        content = str(msg.content)\n",
    "                        print(f\"  -> {content[:200]}...\" if len(content) > 200 else f\"  -> {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 테스트\n",
    "run_agent_stream(\"Multiply 5 and 7, then add 10 to the result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 다른 로컬 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_with_model(model_name: str):\n",
    "    \"\"\"다른 Ollama 모델로 에이전트를 생성합니다.\"\"\"\n",
    "    from langchain_ollama import ChatOllama\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    \n",
    "    local_llm = ChatOllama(\n",
    "        model=model_name,\n",
    "        base_url=OLLAMA_URL,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return create_react_agent(local_llm, tools)\n",
    "\n",
    "\n",
    "# 예시: 다른 모델 사용\n",
    "# other_agent = create_agent_with_model(\"gemma3:12b\")\n",
    "# run_agent(\"Add 5 and 3\", agent=other_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 요약\n",
    "\n",
    "### 로컬 모델 사용의 장점\n",
    "- **프라이버시**: 데이터가 외부로 전송되지 않음\n",
    "- **비용**: API 호출 비용 없음\n",
    "- **오프라인**: 인터넷 연결 없이 사용 가능\n",
    "- **커스터마이징**: 모델 파인튜닝 가능\n",
    "\n",
    "### 로컬 모델 사용 시 고려사항\n",
    "- 추론 속도가 상대적으로 느림 (GPU 필요)\n",
    "- 도구 호출 정확도가 상용 모델보다 낮을 수 있음\n",
    "- 한국어 성능이 제한적일 수 있음\n",
    "- 충분한 RAM/VRAM 필요 (최소 8GB 이상 권장)\n",
    "\n",
    "### 권장 모델 (도구 호출 지원)\n",
    "| 모델 | 크기 | 특징 |\n",
    "|------|------|------|\n",
    "| gemma3:4b | 4B | 경량, 빠름, 도구 호출 지원 |\n",
    "| llama3.1 | 8B | 균형 잡힌 성능 |\n",
    "| llama3.2 | 3B | 경량, 빠름 |\n",
    "| mistral | 7B | 우수한 추론 |\n",
    "| qwen2.5 | 7B | 다국어 지원 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
