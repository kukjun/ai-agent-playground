{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with Ollama & LangGraph\n",
    "\n",
    "LLM ì‘ë‹µì„ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**ìœ¼ë¡œ ë°›ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹\n",
    "\n",
    "| ë°©ì‹ | ì„¤ëª… | ìš©ë„ |\n",
    "|------|------|------|\n",
    "| `astream` | í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° | ê¸°ë³¸ í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° |\n",
    "| `astream_events` | ì´ë²¤íŠ¸ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° | ë„êµ¬ í˜¸ì¶œ, RAG ë“± ìƒì„¸ ì¶”ì  |\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "- Ollama ì„œë²„ ì‹¤í–‰ ì¤‘\n",
    "- `gemma3:12b` ëª¨ë¸ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "MODEL_NAME = \"gemma3:12b\"\n",
    "\n",
    "print(f\"Ollama URL: {OLLAMA_URL}\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_NAME,\n",
    "    base_url=OLLAMA_URL,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"{MODEL_NAME} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° (astream)\n",
    "\n",
    "í† í° ë‹¨ìœ„ë¡œ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°\n",
    "print(\"[ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°]\\n\")\n",
    "\n",
    "for chunk in llm.stream(\"Pythonì˜ ì¥ì  3ê°€ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n[ì™„ë£Œ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°\n",
    "print(\"[ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°]\\n\")\n",
    "\n",
    "async def stream_response():\n",
    "    async for chunk in llm.astream(\"JavaScriptì˜ ì¥ì  3ê°€ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜\"):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\\n[ì™„ë£Œ]\")\n",
    "\n",
    "await stream_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangGraph ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ReAct ì—ì´ì „íŠ¸ì˜ ê° ë‹¨ê³„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information on the web.\"\"\"\n",
    "    # ì‹œë®¬ë ˆì´ì…˜\n",
    "    return f\"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate a math expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"{expression} = {result}\"\n",
    "    except:\n",
    "        return \"ê³„ì‚° ì˜¤ë¥˜\"\n",
    "\n",
    "\n",
    "tools = [search, calculate]\n",
    "\n",
    "# Tool calling ì§€ì› ëª¨ë¸ ì‚¬ìš© (gemma3ëŠ” ì§€ì› ì•ˆ í•¨, qwen3 ì‚¬ìš©)\n",
    "llm_for_agent = ChatOllama(\n",
    "    model=\"qwen3:8b\",  # tool calling ì§€ì› ëª¨ë¸\n",
    "    base_url=OLLAMA_URL,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm_for_agent, tools)\n",
    "print(\"ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream ëª¨ë“œ: ë…¸ë“œ ë‹¨ìœ„ ì—…ë°ì´íŠ¸\n",
    "print(\"[stream ëª¨ë“œ - ë…¸ë“œ ë‹¨ìœ„]\\n\")\n",
    "\n",
    "for event in agent.stream({\"messages\": [(\"human\", \"123 * 456ì„ ê³„ì‚°í•´ì¤˜\")]}):\n",
    "    for node_name, node_output in event.items():\n",
    "        print(f\"=== {node_name} ===\")\n",
    "        if \"messages\" in node_output:\n",
    "            for msg in node_output[\"messages\"]:\n",
    "                if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                    for tc in msg.tool_calls:\n",
    "                        print(f\"  [Tool Call] {tc['name']}({tc['args']})\")\n",
    "                elif msg.content:\n",
    "                    print(f\"  [Content] {msg.content[:100]}...\" if len(str(msg.content)) > 100 else f\"  [Content] {msg.content}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. astream_events (ìƒì„¸ ì´ë²¤íŠ¸)\n",
    "\n",
    "ëª¨ë“  ë‚´ë¶€ ì´ë²¤íŠ¸ë¥¼ ì„¸ë°€í•˜ê²Œ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[astream_events - ìƒì„¸ ì´ë²¤íŠ¸]\\n\")\n",
    "\n",
    "async def stream_events():\n",
    "    async for event in agent.astream_events(\n",
    "        {\"messages\": [(\"human\", \"100 + 200ì„ ê³„ì‚°í•´ì¤˜\")]},\n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        \n",
    "        if kind == \"on_chat_model_start\":\n",
    "            print(f\"ğŸš€ [LLM ì‹œì‘]\")\n",
    "        \n",
    "        elif kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "        \n",
    "        elif kind == \"on_chat_model_end\":\n",
    "            print(f\"\\nâœ… [LLM ì™„ë£Œ]\")\n",
    "        \n",
    "        elif kind == \"on_tool_start\":\n",
    "            print(f\"\\nğŸ”§ [Tool ì‹œì‘] {event['name']}\")\n",
    "            print(f\"   ì…ë ¥: {event['data'].get('input', {})}\")\n",
    "        \n",
    "        elif kind == \"on_tool_end\":\n",
    "            print(f\"ğŸ”§ [Tool ì™„ë£Œ] {event['name']}\")\n",
    "            print(f\"   ê²°ê³¼: {event['data'].get('output', '')}\")\n",
    "\n",
    "await stream_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° (LLMë§Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°]\\n\")\n",
    "print(\"ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "token_count = 0\n",
    "\n",
    "async def stream_tokens():\n",
    "    global token_count\n",
    "    async for chunk in llm.astream(\"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜\"):\n",
    "        token_count += 1\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\n----- ì´ {token_count}ê°œ ì²­í¬ ìˆ˜ì‹  -----\")\n",
    "\n",
    "await stream_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Socket.IO ì„œë²„ ì˜ˆì œ ì½”ë“œ\n",
    "\n",
    "ì‹¤ì œ ì›¹ì†Œì¼“ ì„œë²„ë¥¼ êµ¬ì¶•í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì½”ë“œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Socket.IO ì„œë²„ ì½”ë“œ ì˜ˆì‹œ (ë³„ë„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì‹¤í–‰)\n",
    "\n",
    "server_code = '''\n",
    "import asyncio\n",
    "import socketio\n",
    "from aiohttp import web\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Socket.IO ì„œë²„ ì„¤ì •\n",
    "sio = socketio.AsyncServer(async_mode=\"aiohttp\", cors_allowed_origins=\"*\")\n",
    "app = web.Application()\n",
    "sio.attach(app)\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:12b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "\n",
    "@sio.event\n",
    "async def connect(sid, environ):\n",
    "    print(f\"í´ë¼ì´ì–¸íŠ¸ ì—°ê²°: {sid}\")\n",
    "\n",
    "\n",
    "@sio.event\n",
    "async def disconnect(sid):\n",
    "    print(f\"í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ: {sid}\")\n",
    "\n",
    "\n",
    "@sio.event\n",
    "async def chat(sid, data):\n",
    "    \"\"\"ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\"\"\"\n",
    "    message = data.get(\"message\", \"\")\n",
    "    print(f\"[{sid}] ë©”ì‹œì§€: {message}\")\n",
    "    \n",
    "    try:\n",
    "        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì•Œë¦¼\n",
    "        await sio.emit(\"stream_start\", to=sid)\n",
    "        \n",
    "        # í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        async for chunk in llm.astream(message):\n",
    "            if chunk.content:\n",
    "                await sio.emit(\"token\", {\"content\": chunk.content}, to=sid)\n",
    "        \n",
    "        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì•Œë¦¼\n",
    "        await sio.emit(\"stream_end\", to=sid)\n",
    "    \n",
    "    except Exception as e:\n",
    "        await sio.emit(\"error\", {\"message\": str(e)}, to=sid)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Socket.IO ì„œë²„ ì‹œì‘: http://localhost:8000\")\n",
    "    web.run_app(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "print(\"[Socket.IO ì„œë²„ ì½”ë“œ]\")\n",
    "print(server_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ ì˜ˆì‹œ\n",
    "\n",
    "client_code = '''\n",
    "import asyncio\n",
    "import socketio\n",
    "\n",
    "sio = socketio.AsyncClient()\n",
    "\n",
    "@sio.event\n",
    "async def connect():\n",
    "    print(\"ì„œë²„ì— ì—°ê²°ë¨\")\n",
    "\n",
    "@sio.event\n",
    "async def stream_start():\n",
    "    print(\"\\n[ì‘ë‹µ ì‹œì‘]\")\n",
    "\n",
    "@sio.event\n",
    "async def token(data):\n",
    "    print(data[\"content\"], end=\"\", flush=True)\n",
    "\n",
    "@sio.event\n",
    "async def stream_end():\n",
    "    print(\"\\n[ì‘ë‹µ ì™„ë£Œ]\")\n",
    "\n",
    "@sio.event\n",
    "async def error(data):\n",
    "    print(f\"\\n[ì—ëŸ¬] {data['message']}\")\n",
    "\n",
    "async def main():\n",
    "    await sio.connect(\"http://localhost:8000\")\n",
    "    \n",
    "    # ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡\n",
    "    await sio.emit(\"chat\", {\"message\": \"Pythonì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\"})\n",
    "    \n",
    "    # ì‘ë‹µ ëŒ€ê¸°\n",
    "    await asyncio.sleep(30)\n",
    "    await sio.disconnect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "print(\"[Socket.IO í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ]\")\n",
    "print(client_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangGraph + Socket.IO í†µí•© ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph ì—ì´ì „íŠ¸ + Socket.IO ì„œë²„ ì½”ë“œ\n",
    "\n",
    "agent_server_code = '''\n",
    "import asyncio\n",
    "import socketio\n",
    "from aiohttp import web\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "sio = socketio.AsyncServer(async_mode=\"aiohttp\", cors_allowed_origins=\"*\")\n",
    "app = web.Application()\n",
    "sio.attach(app)\n",
    "\n",
    "# ë„êµ¬ ì •ì˜\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return f\"ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate a math expression.\"\"\"\n",
    "    return str(eval(expression))\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ì„¤ì •\n",
    "llm = ChatOllama(model=\"qwen3:8b\", base_url=\"http://localhost:11434\", temperature=0)\n",
    "agent = create_react_agent(llm, [search, calculate])\n",
    "\n",
    "\n",
    "@sio.event\n",
    "async def chat(sid, data):\n",
    "    message = data.get(\"message\", \"\")\n",
    "    \n",
    "    try:\n",
    "        async for event in agent.astream_events(\n",
    "            {\"messages\": [(\"human\", message)]},\n",
    "            version=\"v2\"\n",
    "        ):\n",
    "            kind = event[\"event\"]\n",
    "            \n",
    "            if kind == \"on_chat_model_stream\":\n",
    "                content = event[\"data\"][\"chunk\"].content\n",
    "                if content:\n",
    "                    await sio.emit(\"token\", {\"content\": content}, to=sid)\n",
    "            \n",
    "            elif kind == \"on_tool_start\":\n",
    "                await sio.emit(\"tool_start\", {\n",
    "                    \"name\": event[\"name\"],\n",
    "                    \"input\": str(event[\"data\"].get(\"input\", {}))\n",
    "                }, to=sid)\n",
    "            \n",
    "            elif kind == \"on_tool_end\":\n",
    "                await sio.emit(\"tool_end\", {\n",
    "                    \"name\": event[\"name\"],\n",
    "                    \"output\": str(event[\"data\"].get(\"output\", \"\"))\n",
    "                }, to=sid)\n",
    "        \n",
    "        await sio.emit(\"done\", to=sid)\n",
    "    \n",
    "    except Exception as e:\n",
    "        await sio.emit(\"error\", {\"message\": str(e)}, to=sid)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Agent + Socket.IO ì„œë²„: http://localhost:8000\")\n",
    "    web.run_app(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "print(\"[LangGraph ì—ì´ì „íŠ¸ + Socket.IO ì„œë²„]\")\n",
    "print(agent_server_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì´ë²¤íŠ¸ íƒ€ì… ì •ë¦¬\n",
    "\n",
    "| ì´ë²¤íŠ¸ | ì„¤ëª… | ë°ì´í„° |\n",
    "|--------|------|--------|\n",
    "| `on_chat_model_start` | LLM í˜¸ì¶œ ì‹œì‘ | ëª¨ë¸ ì •ë³´ |\n",
    "| `on_chat_model_stream` | í† í° ìƒì„± | `chunk.content` |\n",
    "| `on_chat_model_end` | LLM í˜¸ì¶œ ì™„ë£Œ | ì „ì²´ ì‘ë‹µ |\n",
    "| `on_tool_start` | ë„êµ¬ í˜¸ì¶œ ì‹œì‘ | ë„êµ¬ëª…, ì…ë ¥ê°’ |\n",
    "| `on_tool_end` | ë„êµ¬ í˜¸ì¶œ ì™„ë£Œ | ë„êµ¬ëª…, ê²°ê³¼ |\n",
    "| `on_chain_start` | ì²´ì¸ ì‹œì‘ | - |\n",
    "| `on_chain_end` | ì²´ì¸ ì™„ë£Œ | ìµœì¢… ê²°ê³¼ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìš”ì•½\n",
    "\n",
    "### ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì„ íƒ\n",
    "\n",
    "```python\n",
    "# 1. ë‹¨ìˆœ í† í° ìŠ¤íŠ¸ë¦¬ë°\n",
    "async for chunk in llm.astream(\"ì§ˆë¬¸\"):\n",
    "    print(chunk.content)\n",
    "\n",
    "# 2. ë…¸ë“œ ë‹¨ìœ„ ì—…ë°ì´íŠ¸\n",
    "for event in agent.stream(input):\n",
    "    print(event)\n",
    "\n",
    "# 3. ìƒì„¸ ì´ë²¤íŠ¸ (ê¶Œì¥)\n",
    "async for event in agent.astream_events(input, version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content)\n",
    "```\n",
    "\n",
    "### ì›¹ì†Œì¼“ ì—°ë™ ì‹œ\n",
    "- `astream_events`ë¡œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ìº¡ì²˜\n",
    "- ì´ë²¤íŠ¸ íƒ€ì…ë³„ë¡œ ë‹¤ë¥¸ Socket.IO ì´ë²¤íŠ¸ emit\n",
    "- í´ë¼ì´ì–¸íŠ¸ì—ì„œ ê° ì´ë²¤íŠ¸ì— ë§ëŠ” UI ì—…ë°ì´íŠ¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
